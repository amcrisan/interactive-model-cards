{
    "model-details":{
        "name": "Model Details",
        "short": ["This model, `distilbert-base-uncased-finetuned-sst-2-english', is a sentiment analysis"],
        "extended": ["This model is a fine-tune checkpoint of DistilBERT-base-uncased [1], trained using the SST-2 dataset[2]",
            "This model is uncased meaning  it does not make a difference between the words `english’, `English’, and `ENGLISH’",
            "**DistilBERT** is a transformers model, smaller and faster than **BERT**, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts using the BERT base model. This way, the model learns the same inner representation of the English language than its teacher model, while being faster for inference or downstream tasks."
        ]
    },
    "intended-use":{
        "name": "Intended Use",
        "warning" : "Warning! Out of scope uses not reported",
        "short": ["This model is primarily aimed at classifying whether sentences have an overall positive or negative sentiment"
        ],
        "extended": ["This model only reports a probability between 0 and 1 indicating whether some sentence has an overall positive or negative sentiment. The model does not explicitly define a neutral category, but one could be introduced",
        "`Positive sentiment' is represent with the numeric value `1'. A positive sentiment indicates the passage general conveys an happy, confident, or optimistic sentiment",
        "`Negative sentiment' is represent with the numeric value `0'. A negative sentiment indicates the passage general conveys a sad, depressed, or pessimistic sentiment",
        "There are a number known of challenges of correctly determining the [sentiment of text](https://en.wikipedia.org/wiki/Sentiment_analysis)"]

    },
    "ethical-considerations":{
        "name": "Ethical Considerations",
        "warning" : "Warning! Additional bias analysis was not conducted",
        "short": [
            "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of the  **BERT** base model and **DistilBERT-base-uncased**"
        ]
    },
    "training":{
        "name": "Model Training",
        "warning" : "Warning! Dataset is more than five years old",
        "short":  ["This model is fine-tuned using the SST-2, Standford Sentiment Treebank v2 dataset.",
        "This model primarily classifies a sentence as having an overall positive or negative sentiment" ],
        "extended": [
            "distilbert-base-uncased-finetuned-sst-2-english Data : The  SST-2 dataset contains 215,154 phrases with sentiment labels in the parse trees of 11,855 sentences from movie reviews.",
            ["DistilBERT-base-uncased  Data :  BookCorpus [4] , a dataset consisting of 11,038 unpublished books and English Wikipedia [5]. You can learn more about distilbert-base-uncased-finetuned-sst-2-english [here](https://huggingface.co/distilbert-base-uncased)"]
            
        ]
    },
    "references":{
        "name": "References",
        "short" : ["See the research behind the data and models. This research is referenced in other parts of the model card uses the [#] notation."],
        "extended": [
            "[1] Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf.  DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. (2019)",
            "[2]  Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. (2013)",
            "[3]  Jacob Devlin, Ming-Wei Chang, Kenton Leem, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (2018)",
            "[4] https://yknzhu.wixsite.com/mbweb",
            "[5] https://en.wikipedia.org/wiki/English_Wikipedia",
            "[6] Alex Wang , Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding (2019)"
        ]
    }
}